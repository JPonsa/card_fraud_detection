{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# Preprocessing and Model Selection\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to be tested\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set the list of models to compete\n",
    "\n",
    "models = {\n",
    "    \"Logistic Reg.\": LogisticRegression(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(logging_level=\"Silent\"),\n",
    "    # \"Neural Network\": MLPClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    # \"SVM\": SVC(class_weight=\"balanced\", probability=True),\n",
    "    \"K-NN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_path = \"../data/preprocessed/\"\n",
    "\n",
    "train_df = pd.read_csv(preprocessed_path + \"train.csv\")\n",
    "test_df = pd.read_csv(preprocessed_path + \"test.csv\")\n",
    "val_df = pd.read_csv(preprocessed_path + \"validate.csv\")\n",
    "\n",
    "\n",
    "target = [\"TX_FRAUD\"]\n",
    "\n",
    "y_train = train_df[target].values\n",
    "y_val = val_df[target].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "X_train = train_df.drop(target, axis=1)\n",
    "X_val = val_df.drop(target, axis=1)\n",
    "X_test = test_df.drop(target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Augmentation ########################################################\n",
    "# # It is well reported that the dataset is imbalanced. I use SMOTE to increase\n",
    "# # the number of fraud cases in the training set.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.3)\n",
    "under = RandomUnderSampler(sampling_strategy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Create pipelines for each model\n",
    "\n",
    "tag = \"balanced\"\n",
    "pipelines = {}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = imbpipeline(\n",
    "        [\n",
    "            (\"preprocessor\", scaler),\n",
    "            (\"oversampling\", over),\n",
    "            (\"undersampling\", under),\n",
    "            (\"classifier\", model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipelines[model_name] = pipeline\n",
    "\n",
    "scoring = {\n",
    "    \"acc\": \"accuracy\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"auc\": \"roc_auc\",\n",
    "}\n",
    "scores = {}\n",
    "\n",
    "# Perform cross-validation and compare F1 scores\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"########## {model_name} ##########\")\n",
    "\n",
    "    scores[model_name] = cross_validate(\n",
    "        pipeline, X_train, y_train, cv=5, scoring=scoring\n",
    "    )\n",
    "    f1_scores = scores[model_name][\"test_f1\"]\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    print(f\"{model_name} - Mean F1 score: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the scoring so it can be plotted\n",
    "# fmt:off\n",
    "metrics = pd.DataFrame([], columns=[ \"fit_time\", \"score_time\", \"test_acc\", \"test_f1\",\n",
    "                                    \"test_recall\", \"test_precision\", \"model_name\"])\n",
    "\n",
    "# fmt:on\n",
    "\n",
    "for model_name in scores.keys():\n",
    "    tmp = pd.DataFrame(scores[model_name])\n",
    "    tmp[\"model_name\"] = model_name\n",
    "\n",
    "    metrics = pd.concat([metrics, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot competition results\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, sharey=True)\n",
    "\n",
    "for (i, metric), ylabel in zip(\n",
    "    enumerate([\"test_f1\", \"test_precision\", \"test_recall\", \"test_acc\"]),\n",
    "    [\"F1\", \"Precision\", \"Recall\", \"Acc\"],\n",
    "):\n",
    "    sns.pointplot(data=metrics, x=metric, linestyle=\"none\", y=\"model_name\", ax=ax[i])\n",
    "    ax[i].set_ylabel(None)\n",
    "    ax[i].set_xlabel(ylabel)\n",
    "    ax[i].set_xlim([0, 1])\n",
    "\n",
    "    ranking = metrics.groupby(\"model_name\")[metric].mean().sort_values(ascending=False)\n",
    "\n",
    "    ax[i].scatter(\n",
    "        ranking.values[0],\n",
    "        list(models).index(ranking.index[0]),\n",
    "        color=\"r\",\n",
    "        marker=\"o\",\n",
    "        s=150,\n",
    "    )\n",
    "\n",
    "fig.suptitle(f\"Model Selection - {tag} dataset\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"../figures/model_selection_{tag}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ranking = (\n",
    "    metrics.groupby(\"model_name\").mean().sort_values(by=\"test_f1\", ascending=False)\n",
    ")\n",
    "f1_winner = f1_ranking.index[0]\n",
    "\n",
    "f1_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have a DataFrame df\n",
    "dfi.export(\n",
    "    f1_ranking, f\"../figures/f1_ranking_{tag}.png\", table_conversion=\"matplotlib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines[f1_winner].fit(X_train, y_train)\n",
    "y_pred = pipelines[f1_winner].predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "report = pd.DataFrame(\n",
    "    classification_report(y_test, y_pred, output_dict=True)\n",
    ").transpose()\n",
    "dfi.export(report, f\"../figures/default.catBoost.png\", table_conversion=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(5, 3))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax[0], colorbar=False)\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    normalize=\"true\",\n",
    "    values_format=\".2f\",\n",
    "    ax=ax[1],\n",
    "    im_kw={\"vmin\": 0, \"vmax\": 1},\n",
    "    colorbar=False,\n",
    ")\n",
    "fig.suptitle(\"Confusion Matrix\")\n",
    "ax[0].set_title(\"Counts\")\n",
    "ax[1].set_title(\"Proportions\")\n",
    "ax[1].set_ylabel(None)\n",
    "fig.savefig(f\"../figures/default.catBoost.cm.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
